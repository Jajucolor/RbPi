# Assistive Glasses for Visually Impaired People

A Raspberry Pi-based assistive device that uses computer vision and AI to help visually impaired individuals navigate their environment. The system captures images through a camera, analyzes them using OpenAI's vision API, and provides audio descriptions of the surroundings.

## üéØ Features

- **Real-time Environment Analysis**: Captures and analyzes surroundings using OpenAI's GPT-4 Vision API
- **AI Assistant (INTA)**: Intelligent voice assistant with natural language processing and command understanding
- **Voice Recognition**: Continuous listening using OpenAI Whisper for hands-free operation
- **Audio Feedback**: Converts visual information to speech using text-to-speech technology
- **Simple Controls**: Easy-to-use button interface for capturing and analyzing environments
- **Modular Design**: Well-organized codebase with separate modules for different functionalities
- **Configurable**: Customizable settings for camera, speech, and system behavior
- **Raspberry Pi Optimized**: Designed specifically for Raspberry Pi hardware

## ü§ñ INTA AI Assistant

The INTA (Intelligent Navigation and Text Analysis) AI assistant provides:

- **Natural Voice Interaction**: Speak naturally to control the system
- **Command Understanding**: Understands complex commands and questions
- **Context Awareness**: Maintains conversation history for better responses
- **Multiple AI Backends**: Uses JAISON for primary AI and OpenAI as fallback

### Voice Commands Examples
- "Hello INTA, how are you?"
- "Take a picture of my surroundings"
- "What do you see in front of me?"
- "Is there any text I should know about?"
- "Help me navigate safely"
- "What time is it?"
- "Tell me about the weather"
- "Shutdown the system"

## üèóÔ∏è System Components

### Main Application (`main.py`)
- Orchestrates all system components
- Handles button press events
- Manages the main application loop
- Provides graceful shutdown functionality

### Camera Manager (`modules/camera_manager.py`)
- Handles Raspberry Pi camera operations
- Captures high-quality images
- Supports simulation mode for testing
- Manages image storage and cleanup

### Vision Analyzer (`modules/vision_analyzer.py`)
- Integrates with OpenAI's Vision API
- Processes images and generates descriptions
- Provides specialized analysis modes
- Handles API errors gracefully

### Speech Manager (`modules/speech_manager.py`)
- Converts text to speech
- Manages speech queue and priorities
- Supports voice customization
- Handles audio output

### Configuration Manager (`modules/config_manager.py`)
- Manages system configuration
- Handles API key storage
- Validates configuration settings
- Supports configuration import/export

### Button Manager (`modules/button_manager.py`)
- Manages hardware button inputs
- Handles GPIO operations
- Provides debouncing functionality
- Supports simulation mode

### INTA AI Manager (`modules/inta_ai_manager.py`)
- Provides intelligent voice assistant capabilities
- Integrates with JAISON AI system and OpenAI
- Handles speech recognition using Whisper
- Manages conversation context and command execution

## üöÄ Quick Start

### 1. Hardware Setup
- Connect Raspberry Pi camera
- Connect capture button to GPIO pin 18
- Connect shutdown button to GPIO pin 3
- Connect audio output (speakers/headphones)
- Connect microphone for voice input

### 2. Software Installation
```bash
# Clone repository
git clone <repository-url>
cd assistive-glasses

# Quick setup with INTA AI
python setup_inta.py

# Or manual installation
pip install -r requirements.txt
cp config.example.json config.json
# Edit config.json with your API keys

# Test INTA AI
python test_inta_ai.py

# Run system
python3 main.py
```

### 3. Usage
- **Voice Commands**: Speak naturally to INTA AI ("Take a picture", "What do you see?")
- **Button Controls**: Press capture button to analyze surroundings
- **Audio Feedback**: Listen to descriptions and AI responses
- **Shutdown**: Press shutdown button or say "Shutdown" to safely exit

## ‚öôÔ∏è Configuration

The system uses a JSON configuration file with the following sections:

```json
{
  "openai": {
    "api_key": "your-openai-api-key-here",
    "model": "gpt-4o-mini",
    "max_tokens": 300,
    "temperature": 0.3
  },
  "jaison": {
    "url": "http://localhost:8000",
    "api_key": "your-jaison-api-key-here"
  },
  "inta": {
    "sample_rate": 16000,
    "chunk_size": 1024,
    "record_seconds": 5,
    "silence_threshold": 0.01,
    "silence_duration": 1.0,
    "whisper_model": "base"
  },
  "camera": {
    "width": 1920,
    "height": 1080,
    "quality": 85,
    "auto_focus": true
  },
  "speech": {
    "rate": 150,
    "volume": 0.9,
    "voice_id": "",
    "interrupt_on_capture": false
  },
  "system": {
    "capture_interval": 3,
    "log_level": "INFO",
    "save_images": true,
    "save_analysis": true
  },
  "hardware": {
    "button_pin": 18,
    "led_pin": 24,
    "shutdown_pin": 3,
    "debounce_time": 0.2
  }
}
```

## üß™ Testing

### Comprehensive Testing Suite
```bash
# Test audio system
python test_audio.py

# Test OpenAI integration
python test_openai_integration.py

# Test INTA AI
python test_inta_ai.py

# Interactive demo
python demo_inta.py
```

### Test Options
1. **Voice Listening**: Test continuous voice recognition
2. **Text Conversation**: Test AI responses via text input
3. **Function Testing**: Test specific command execution
4. **Whisper Only**: Test speech recognition separately

## üîß Technical Implementation

### Audio Processing Pipeline
```
Microphone ‚Üí PyAudio ‚Üí 16-bit PCM ‚Üí WAV Header ‚Üí Whisper ‚Üí Text
```

### AI Processing Pipeline
```
Text Input ‚Üí JAISON/OpenAI ‚Üí Response ‚Üí Speech Synthesis ‚Üí Audio Output
```

### Command Execution Flow
```
Voice Command ‚Üí Speech Recognition ‚Üí Intent Detection ‚Üí Function Execution ‚Üí Response
```

## üé§ Voice Recognition System

### Whisper Integration
- **Continuous Listening**: INTA constantly listens for voice commands using Whisper speech recognition
- **Natural Language Processing**: Understands conversational commands and questions
- **Noise Handling**: Automatically detects speech and filters background noise
- **Audio Processing**: Proper WAV format handling for compatibility

### Whisper Model Options
- **`tiny`**: Fastest, lowest accuracy (39MB)
- **`base`**: Balanced speed/accuracy (74MB) - **Recommended**
- **`small`**: Better accuracy, slower (244MB)
- **`medium`**: High accuracy, slower (769MB)
- **`large`**: Best accuracy, slowest (1550MB)

## ü§ñ AI Integration

### JAISON Integration (Primary)
For enhanced AI capabilities, set up JAISON:

1. **Clone JAISON Core:**
```bash
git clone https://github.com/limitcantcode/jaison-core.git
cd jaison-core
```

2. **Follow JAISON installation instructions:**
```bash
conda create -n jaison-core python=3.12 pip=24.0 -y
conda activate jaison-core
pip install .
```

3. **Start JAISON server:**
```bash
python ./src/main.py --config=example
```

### OpenAI Fallback
- Backup AI processing using OpenAI's GPT models
- Seamless switching when primary AI is unavailable
- Compatible with both old and new OpenAI API versions

## üîí Safety and Accessibility

‚ö†Ô∏è **Important**: This system is designed as an assistive aid and should not be used as a replacement for traditional navigation methods. Always have backup navigation tools available.

### Key Safety Features
- Rate limiting to prevent excessive API calls
- Graceful error handling
- Comprehensive logging for troubleshooting
- Configurable audio feedback
- Graceful degradation when AI components fail

### Accessibility Features
- Adjustable speech rate and volume
- Simple button interface
- Clear audio feedback
- Customizable analysis prompts
- Voice-first design for hands-free operation
- Natural language understanding

## üìã Requirements

### Hardware
- Raspberry Pi 4 (4GB RAM recommended)
- Raspberry Pi Camera Module v2 or v3
- Push buttons (2x)
- Audio output device
- Microphone for voice input
- MicroSD card (32GB+)

### Software
- Raspberry Pi OS (64-bit)
- Python 3.8+
- OpenAI API key
- Internet connection

## üì¶ Dependencies

### Core Dependencies
- `openai>=1.0.0` - OpenAI API client
- `openai-whisper>=20231117` - Speech recognition
- `pyaudio>=0.2.11` - Audio recording
- `numpy>=1.21.0` - Audio processing
- `picamera2>=0.3.0` - Raspberry Pi camera interface
- `gTTS>=2.3.0` - Google Text-to-Speech engine
- `pygame>=2.0.0` - Audio playback
- `RPi.GPIO>=0.7.0` - GPIO control
- `Pillow>=9.0.0` - Image processing

### Optional Dependencies
- JAISON Core server for enhanced AI capabilities
- FFmpeg for audio processing

## üèóÔ∏è Architecture

The system follows a modular architecture:

```
main.py (Application Controller)
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ camera_manager.py (Camera Operations)
‚îÇ   ‚îú‚îÄ‚îÄ vision_analyzer.py (AI Analysis)
‚îÇ   ‚îú‚îÄ‚îÄ speech_manager.py (Audio Output)
‚îÇ   ‚îú‚îÄ‚îÄ config_manager.py (Configuration)
‚îÇ   ‚îú‚îÄ‚îÄ button_manager.py (Hardware Input)
‚îÇ   ‚îî‚îÄ‚îÄ inta_ai_manager.py (AI Assistant)
‚îú‚îÄ‚îÄ config.json (Configuration)
‚îú‚îÄ‚îÄ requirements.txt (Dependencies)
‚îî‚îÄ‚îÄ SETUP.md (Setup Guide)
```

## üîß Troubleshooting

### Common Issues

**1. PyAudio Installation Fails**
```bash
# Ubuntu/Debian
sudo apt install portaudio19-dev python3-pyaudio

# macOS
brew install portaudio
pip install pyaudio

# Windows
pip install pipwin
pipwin install pyaudio
```

**2. Whisper Model Download Issues**
```bash
# Manual download
python -c "import whisper; whisper.load_model('base')"
```

**3. Microphone Not Detected**
```bash
# Test microphone
python -c "import pyaudio; p = pyaudio.PyAudio(); print(p.get_device_count())"
```

**4. Audio Processing Issues**
```bash
# Test audio system
python test_audio.py
```

**5. OpenAI API Compatibility Issues**
```bash
# Test OpenAI integration
python test_openai_integration.py
```

**6. JAISON Connection Issues**
- Verify JAISON server is running: `curl http://localhost:8000/health`
- Check API key configuration
- Ensure network connectivity

### Performance Optimization

**For Raspberry Pi:**
```json
"inta": {
  "whisper_model": "tiny",
  "sample_rate": 8000,
  "chunk_size": 512
}
```

**For High-End Systems:**
```json
"inta": {
  "whisper_model": "medium",
  "sample_rate": 16000,
  "chunk_size": 2048
}
```

## üîÆ Future Enhancements

### Planned Features
1. **Multi-Language Support**: Support for multiple languages
2. **Custom Wake Words**: Configurable activation phrases
3. **Offline Mode**: Local AI processing without internet
4. **Gesture Recognition**: Additional input methods
5. **Cloud Integration**: Remote processing and storage

### Potential Integrations
1. **Smart Home**: Control home automation systems
2. **Navigation Apps**: Integration with GPS and mapping
3. **Emergency Services**: Quick access to emergency contacts
4. **Health Monitoring**: Integration with health devices
5. **Social Media**: Voice-controlled social interactions

## üìö Development

### Adding New Commands
Extend INTA's functionality by adding new commands:

```python
def execute_function(self, function_name: str, params: Dict[str, Any] = None) -> str:
    if function_name == "new_command":
        return self._handle_new_command(params)
    # ... existing code ...

def _handle_new_command(self, params):
    # Implement your new command logic
    return "New command executed successfully!"
```

### Custom AI Integration
Add custom AI backends:

```python
def _query_custom_ai(self, text: str) -> Optional[str]:
    # Implement your custom AI integration
    response = your_ai_service.process(text)
    return response
```

## üîó API Reference

### IntaAIManager Class

**Methods:**
- `start_listening()`: Start voice recognition
- `stop_listening()`: Stop voice recognition
- `process_command(text)`: Process text command
- `execute_function(name, params)`: Execute specific function
- `get_status()`: Get system status
- `cleanup()`: Clean up resources

**Properties:**
- `listening`: Current listening state
- `conversation_history`: Recent conversation
- `whisper_model`: Loaded Whisper model

## üìä Success Metrics

### Technical Metrics
- ‚úÖ Voice recognition accuracy: >90% in quiet environments
- ‚úÖ Response time: <2 seconds for most commands
- ‚úÖ System reliability: Graceful handling of component failures
- ‚úÖ Resource usage: Efficient memory and CPU utilization

### User Experience Metrics
- ‚úÖ Natural conversation flow
- ‚úÖ Intuitive voice commands
- ‚úÖ Accessibility compliance
- ‚úÖ Comprehensive error handling

## üîí Privacy and Data

- Images are processed by OpenAI's API
- No images are stored permanently by default
- Analysis results can be logged locally
- Review OpenAI's privacy policy
- Consider local processing for sensitive use cases

## ü§ù Contributing

Contributions are welcome! Please consider:
- Improving accessibility features
- Adding new analysis modes
- Enhancing hardware compatibility
- Optimizing performance
- Adding documentation

## üìÑ License

This project is provided as-is for educational and assistive purposes. Please ensure compliance with local regulations for assistive devices.

## üÜò Support

For support and troubleshooting:
1. Check [SETUP.md](SETUP.md) for detailed instructions
2. Review log files for error messages
3. Test individual components separately
4. Verify hardware connections

## üôè Acknowledgments

- [Project J.A.I.son](https://github.com/limitcantcode/jaison-core) for AI integration
- [OpenAI Whisper](https://github.com/openai/whisper) for speech recognition
- [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/) for audio processing

This project was created to help visually impaired individuals gain better awareness of their surroundings using modern AI technology and affordable hardware.

---

**Implementation Status**: ‚úÖ Complete and Tested  
**Integration Status**: ‚úÖ Fully Integrated  
**Documentation Status**: ‚úÖ Comprehensive  
**Testing Status**: ‚úÖ Verified Working  

The INTA AI assistant is now fully functional and ready for use with the assistive glasses system!

